{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c13f29-ce54-430c-a5a2-28678c8d9c52",
   "metadata": {},
   "source": [
    "# **4.3 Least Square Approximations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc767b89-b4ba-4159-99a0-0cc426af407f",
   "metadata": {},
   "source": [
    "- When $Ax = b$ has no solution, multiply by $A^T$ and solve  \n",
    "$$\n",
    "A^T A \\hat{x} = A^T b\n",
    "$$\n",
    "\n",
    "### **Minimizing the Error**\n",
    "\n",
    "- **Question:** How do we make the error $e = b - Ax$ as small as possible?\n",
    "  - **Geometry:** The best $\\hat{x}$ can be found by geometry â€” the error $e$ meets the column space of $A$ at $90^\\circ$.\n",
    "  - **Algebra:** Solve $A^T A \\hat{x} = A^T b$.\n",
    "  - **Calculus:** The derivative of the error $\\|Ax-b\\|^2$ is zero at $\\hat{x}$.\n",
    "\n",
    "- The solution to $A\\hat{x} = p$ leaves the least possible error $e$.  \n",
    "$$\n",
    "\\|Ax - b\\|^2 = \\|Ax - p\\|^2 + \\|e\\|^2\n",
    "$$\n",
    "\n",
    "- The partial derivatives of $\\|Ax-b\\|^2$ are zero when $A^T A \\hat{x} = A^T b$.\n",
    "\n",
    "### **The Big Picture for Least Squares**\n",
    "\n",
    "- There are no exact solutions to $Ax = b$. Instead of splitting $x$, we split $b = p + e$.\n",
    "\n",
    "### **Fitting a Straight Line**\n",
    "\n",
    "- Fitting a line is the clearest application of least squares. For $m>2$ points $(t_1, b_1), \\dots, (t_m, b_m)$, the best line $C + Dt$ minimizes vertical distances $e_1, \\dots, e_m$:  \n",
    "$$\n",
    "E = e_1^2 + \\dots + e_m^2\n",
    "$$\n",
    "\n",
    "- A line goes through all points when we exactly solve $Ax = b$. Generally, we can't do this.  \n",
    "  Two unknowns $C$ and $D$ determine a line, so $A$ has only $n = 2$ columns. To fit $m$ points, we try to solve $m$ equations (and only have 2 unknowns!):\n",
    "\n",
    "$$\n",
    "Ax = b, \\quad \n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "1 & t_1 \\\\\n",
    "1 & t_2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & t_m\n",
    "\\end{bmatrix}, \\quad \n",
    "b = \n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- The column space is usually thin, so $b$ is generally outside of it. When $b$ lies in the column space, $b = p$ and $e = 0$.  \n",
    "\n",
    "- The closest line $C + Dt$ has heights $p_1, \\dots, p_m$ with errors $e_1, \\dots, e_m$. Solve  \n",
    "$$\n",
    "A^T A \\hat{x} = A^T b, \\quad \\hat{x} = (C, D)\n",
    "$$  \n",
    "The errors: $e_i = b_i - C - Dt_i$\n",
    "\n",
    "- Normal equations explicitly:  \n",
    "$$\n",
    "A^T A = \n",
    "\\begin{bmatrix}\n",
    "1 & \\dots & 1 \\\\\n",
    "t_1 & \\dots & t_m\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "1 & t_1 \\\\ \\vdots & \\vdots \\\\ 1 & t_m\n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "m & \\sum t_i \\\\\n",
    "\\sum t_i & \\sum t_i^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^T b = \n",
    "\\begin{bmatrix}\n",
    "1 & \\dots & 1 \\\\\n",
    "t_1 & \\dots & t_m\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\ \\vdots \\\\ b_m\n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "\\sum b_i \\\\ \\sum t_i b_i\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Solve for $\\hat{x} = (C, D)$ using  \n",
    "$$\n",
    "\\hat{x} = (A^T A)^{-1} A^T b\n",
    "$$\n",
    "\n",
    "- The vertical errors (residuals) $e = b - A \\hat{x}$ are perpendicular to the columns of $A$ (geometry) and lie in the nullspace of $A^T$ (linear algebra). The best $\\hat{x} = (C, D)$ minimizes the total squared error:  \n",
    "$$\n",
    "E(x) = \\|Ax - b\\|^2 = (C + Dt_1 - b_1)^2 + \\dots + (C + Dt_m - b_m)^2\n",
    "$$\n",
    "\n",
    "**Key Ideas**\n",
    "\n",
    "1. The least squares solution $\\hat{x}$ minimizes  \n",
    "$$\n",
    "\\|Ax-b\\|^2 = x^T A^T A x - 2 x^T A^T b + b^T b\n",
    "$$  \n",
    "This is the sum of squares of errors for $m$ equations ($m>n$).\n",
    "\n",
    "2. The best $\\hat{x}$ comes from the normal equation $A^T A \\hat{x} = A^T b$.\n",
    "\n",
    "3. To fit $m$ points by a line $b = C + Dt$, the normal equations give $C$ and $D$.\n",
    "\n",
    "4. Heights of the best line: $p = (p_1, \\dots, p_m)$. Errors: $e = (e_1, \\dots, e_m)$. Key equation:  \n",
    "$$\n",
    "A^T e = 0\n",
    "$$\n",
    "\n",
    "5. To fit $m$ points by a combination of $n < m$ functions, $Ax = b$ is generally unsolvable. The least squares solution comes from  \n",
    "$$\n",
    "A^T A \\hat{x} = A^T b\n",
    "$$  \n",
    "yielding the combination with smallest mean squared error (MSE).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
