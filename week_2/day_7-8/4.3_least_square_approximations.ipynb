{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c13f29-ce54-430c-a5a2-28678c8d9c52",
   "metadata": {},
   "source": [
    "# **4.3 Least Square Approximations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc767b89-b4ba-4159-99a0-0cc426af407f",
   "metadata": {},
   "source": [
    "- When $Ax = b$ has no solution, multiply by $A^T$ and solve  \n",
    "$$\n",
    "A^T A \\hat{x} = A^T b\n",
    "$$\n",
    "\n",
    "### **Minimizing the Error**\n",
    "\n",
    "- **Question:** How do we make the error $e = b - Ax$ as small as possible?\n",
    "  - **Geometry:** The best $\\hat{x}$ can be found by geometry — the error $e$ meets the column space of $A$ at $90^\\circ$.\n",
    "  - **Algebra:** Solve $A^T A \\hat{x} = A^T b$.\n",
    "  - **Calculus:** The derivative of the error $\\|Ax-b\\|^2$ is zero at $\\hat{x}$.\n",
    "\n",
    "- The solution to $A\\hat{x} = p$ leaves the least possible error $e$.  \n",
    "$$\n",
    "\\|Ax - b\\|^2 = \\|Ax - p\\|^2 + \\|e\\|^2\n",
    "$$\n",
    "\n",
    "- The partial derivatives of $\\|Ax-b\\|^2$ are zero when $A^T A \\hat{x} = A^T b$.\n",
    "\n",
    "### **The Big Picture for Least Squares**\n",
    "\n",
    "- There are no exact solutions to $Ax = b$. Instead of splitting $x$, we split $b = p + e$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6034ff8-0e04-4792-b30f-b3d7e4612783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 1.200000000000001\n",
      "D: 0.5999999999999999\n",
      "predictions: [1.2 1.8 2.4 3.  3.6]\n",
      "residuals: [-0.2  0.2 -0.4  1.  -0.6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def least_squares_error(A, b):\n",
    "    # Compute projection onto Col(A)\n",
    "    P = A @ np.linalg.inv(A.T @ A) @ A.T\n",
    "    # Compute minimum-error residual\n",
    "    e = b - P @ b\n",
    "    return e, np.linalg.norm(e), np.linalg.norm(e)**2\n",
    "\n",
    "\n",
    "A = np.array([\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [3, 4]\n",
    "], dtype=float)\n",
    "\n",
    "b = np.array([3, 1, 2], dtype=float)\n",
    "\n",
    "e_vec, e_norm, e_norm_sq = least_squares_error(A, b)\n",
    "\n",
    "print(\"Residual vector e:\", e_vec)\n",
    "print(\"Minimum error ||e||:\", e_norm)\n",
    "print(\"Minimum squared error ||e||^2:\", e_norm_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d90cd-c8c0-4eba-8168-c537cf39f953",
   "metadata": {},
   "source": [
    "### **Fitting a Straight Line**\n",
    "\n",
    "- Fitting a line is the clearest application of least squares. For $m>2$ points $(t_1, b_1), \\dots, (t_m, b_m)$, the best line $C + Dt$ minimizes vertical distances $e_1, \\dots, e_m$:  \n",
    "$$\n",
    "E = e_1^2 + \\dots + e_m^2\n",
    "$$\n",
    "\n",
    "- A line goes through all points when we exactly solve $Ax = b$. Generally, we can't do this.  \n",
    "  Two unknowns $C$ and $D$ determine a line, so $A$ has only $n = 2$ columns. To fit $m$ points, we try to solve $m$ equations (and only have 2 unknowns!):\n",
    "\n",
    "$$\n",
    "Ax = b, \\quad \n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "1 & t_1 \\\\\n",
    "1 & t_2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & t_m\n",
    "\\end{bmatrix}, \\quad \n",
    "b = \n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- The column space is usually thin, so $b$ is generally outside of it. When $b$ lies in the column space, $b = p$ and $e = 0$.  \n",
    "\n",
    "- The closest line $C + Dt$ has heights $p_1, \\dots, p_m$ with errors $e_1, \\dots, e_m$. Solve  \n",
    "$$\n",
    "A^T A \\hat{x} = A^T b, \\quad \\hat{x} = (C, D)\n",
    "$$  \n",
    "The errors: $e_i = b_i - C - Dt_i$\n",
    "\n",
    "- Normal equations explicitly:  \n",
    "$$\n",
    "A^T A = \n",
    "\\begin{bmatrix}\n",
    "1 & \\dots & 1 \\\\\n",
    "t_1 & \\dots & t_m\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "1 & t_1 \\\\ \\vdots & \\vdots \\\\ 1 & t_m\n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "m & \\sum t_i \\\\\n",
    "\\sum t_i & \\sum t_i^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^T b = \n",
    "\\begin{bmatrix}\n",
    "1 & \\dots & 1 \\\\\n",
    "t_1 & \\dots & t_m\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\ \\vdots \\\\ b_m\n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "\\sum b_i \\\\ \\sum t_i b_i\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Solve for $\\hat{x} = (C, D)$ using  \n",
    "$$\n",
    "\\hat{x} = (A^T A)^{-1} A^T b\n",
    "$$\n",
    "\n",
    "- The vertical errors (residuals) $e = b - A \\hat{x}$ are perpendicular to the columns of $A$ (geometry) and lie in the nullspace of $A^T$ (linear algebra). The best $\\hat{x} = (C, D)$ minimizes the total squared error:  \n",
    "$$\n",
    "E(x) = \\|Ax - b\\|^2 = (C + Dt_1 - b_1)^2 + \\dots + (C + Dt_m - b_m)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8760fa9d-a289-44c0-8e9c-d76dc42a2d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_hat: 0.3333333333333333\n",
      "projection p: [1.66666667 0.33333333 1.66666667]\n",
      "residual e: [0.33333333 0.66666667 1.33333333]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit_line_scalar(C, D, b):\n",
    "    \"\"\"\n",
    "    Solve C + D t ≈ b in the least-squares sense.\n",
    "    \n",
    "    Returns:\n",
    "        t_hat: best-fit scalar\n",
    "        p: projection point C + D t_hat\n",
    "        e: residual vector b - p\n",
    "    \"\"\"\n",
    "    C = np.asarray(C, dtype=float)\n",
    "    D = np.asarray(D, dtype=float)\n",
    "    b = np.asarray(b, dtype=float)\n",
    "\n",
    "    t_hat = D @ (b - C) / (D @ D)\n",
    "    p = C + D * t_hat\n",
    "    e = b - p\n",
    "    \n",
    "    return t_hat, p, e\n",
    "\n",
    "C = np.array([1.0, 0.0, 2.0])\n",
    "D = np.array([2.0, 1.0, -1.0])\n",
    "b = np.array([2.0, 1.0, 3.0])\n",
    "\n",
    "t_hat, p, e = fit_line_scalar(C, D, b)\n",
    "\n",
    "print(\"t_hat:\", t_hat)\n",
    "print(\"projection p:\", p)\n",
    "print(\"residual e:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e0d45-d391-428c-a549-55fbd69ab06b",
   "metadata": {},
   "source": [
    "**Key Ideas**\n",
    "\n",
    "1. The least squares solution $\\hat{x}$ minimizes  \n",
    "$$\n",
    "\\|Ax-b\\|^2 = x^T A^T A x - 2 x^T A^T b + b^T b\n",
    "$$  \n",
    "This is the sum of squares of errors for $m$ equations ($m>n$).\n",
    "\n",
    "2. The best $\\hat{x}$ comes from the normal equation $A^T A \\hat{x} = A^T b$.\n",
    "\n",
    "3. To fit $m$ points by a line $b = C + Dt$, the normal equations give $C$ and $D$.\n",
    "\n",
    "4. Heights of the best line: $p = (p_1, \\dots, p_m)$. Errors: $e = (e_1, \\dots, e_m)$. Key equation:  \n",
    "$$\n",
    "A^T e = 0\n",
    "$$\n",
    "\n",
    "5. To fit $m$ points by a combination of $n < m$ functions, $Ax = b$ is generally unsolvable. The least squares solution comes from  \n",
    "$$\n",
    "A^T A \\hat{x} = A^T b\n",
    "$$  \n",
    "yielding the combination with smallest mean squared error (MSE).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
